{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TsvetaIvanova/ColabIMLO/blob/main/OffIMLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l08PFO2OOMwh"
      },
      "source": [
        "Running instructions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjwdNcnSYDAB",
        "outputId": "6ec65c0f-6bbb-4584-c612-cbd29d097327"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.3.0+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchviz)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 516, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 587, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\n",
            "    file = get_http_url(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 109, in get_http_url\n",
            "    hashes.check_against_path(from_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/hashes.py\", line 106, in check_against_path\n",
            "    return self.check_against_file(file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/hashes.py\", line 102, in check_against_file\n",
            "    return self.check_against_chunks(read_chunks(file))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/hashes.py\", line 86, in check_against_chunks\n",
            "    hash.update(chunk)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1514, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7VtSU6COScK"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ekt1a48sP2_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "46596769-012e-42dd-ff91-049cddc31abd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchviz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-261601d0db70>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor, transforms\n",
        "import time\n",
        "from PIL import Image\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import lr_scheduler\n",
        "from google.colab import runtime\n",
        "from google.colab import drive\n",
        "from torchsummary import summary\n",
        "from torchviz import make_dot\n",
        "from torchvision.transforms import v2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwgkPUqROJ4y"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQDKWFP-TnvE"
      },
      "source": [
        "#Get full dataset into dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1V1fZRvOK1G"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0CfvlGlgQMy"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)\n",
        "# !pip install torch==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbLccXu4QTxr"
      },
      "source": [
        "# Get and process data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU9o9D2T69fX"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sMfEGZDmvL4"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_train_transform = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.Resize(256),\n",
        "    v2.CenterCrop(224),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomRotation(50),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "data_val_transform = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.Resize(256),\n",
        "    v2.CenterCrop(224),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomRotation(50),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "data_test_transform = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.Resize(256),\n",
        "    v2.CenterCrop(224),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lZyZ2SpReNV"
      },
      "outputs": [],
      "source": [
        "def show_image(dataset, index=0):\n",
        "    image, label = dataset[index]\n",
        "    image = (image * torch.tensor([0.229, 0.224, 0.225])[:, None, None]) + torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
        "    image = image.clamp(0, 1)\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "    plt.title(f'Label: {label}')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv8mUlZwocLv"
      },
      "outputs": [],
      "source": [
        "train_set = datasets.Flowers102(root='data', split='train', download=True, transform=data_train_transform)\n",
        "test_set = datasets.Flowers102(root='data', split='test', download=True, transform=data_test_transform)\n",
        "val_set = datasets.Flowers102(root='data', split='val', download=True, transform=data_val_transform)\n",
        "\n",
        "\n",
        "# creating data loaders\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOyROY8P356M"
      },
      "outputs": [],
      "source": [
        "show_image(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVqetB6dodR1"
      },
      "source": [
        "## using the datasets.Flowers102 function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4NZf_gGZBXI"
      },
      "source": [
        "# preprocess data for training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBVjF1c1x-jM"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RujhkS8g3P"
      },
      "source": [
        "# NN Architecture(Neural Network)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSy5B5ndV1m0"
      },
      "source": [
        "## define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRCSc9mA8pTE"
      },
      "outputs": [],
      "source": [
        "#a second dropout to the best performing one 52.6%\n",
        "\n",
        "# class ConvNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(ConvNN, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(64)\n",
        "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(128)\n",
        "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(256)\n",
        "#         self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "#         self.bn4 = nn.BatchNorm2d(512)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "#         self.bn5 = nn.BatchNorm1d(1024)\n",
        "#         self.dropout1 = nn.Dropout(0.5)\n",
        "#         self.fc2 = nn.Linear(1024, 102)\n",
        "#         self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "#         x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "#         x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "#         x = x.view(-1, 512 * 14 * 14)\n",
        "#         x = F.relu(self.bn5(self.fc1(x)))\n",
        "#         x = self.dropout1(x)\n",
        "#         x = self.dropout2(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# adding 2 more fc\n",
        "\n",
        "class ConvNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 2048)\n",
        "        self.bn5 = nn.BatchNorm1d(2048)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(2048, 1024)\n",
        "        self.bn6 = nn.BatchNorm1d(1024)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(1024, 512)\n",
        "        self.bn7 = nn.BatchNorm1d(512)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc4 = nn.Linear(512, 102)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "\n",
        "        x = F.relu(self.bn5(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.bn6(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.bn7(self.fc3(x)))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swfW5JeRWtqy"
      },
      "outputs": [],
      "source": [
        "  # Training function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = loss_fn(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f'Epoch {t+1}, Batch {batch}, Loss: {loss.item():.4f}')\n",
        "        if batch == 0:\n",
        "            start_time = time.time()\n",
        "        elif batch == 1:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Time per batch: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    print(f'Average Loss: {average_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL97j7H8W-IY"
      },
      "outputs": [],
      "source": [
        "# Testing function\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtCQyr07Wbq7"
      },
      "outputs": [],
      "source": [
        "model = ConvNN().to(device)\n",
        "summary(model, (3, 224, 224))\n",
        "print(model)\n",
        "model.eval()\n",
        "x = torch.randn(1, 3, 224, 224).to(device)\n",
        "y = model(x)\n",
        "dot = make_dot(y, params=dict(model.named_parameters()))\n",
        "dot.format = 'png'\n",
        "dot.render('model_architecture')\n",
        "\n",
        "# LossFN\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)  # Added L2 regularization\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AElWXT8OtyfR"
      },
      "source": [
        "# Train and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pBum03crxh3"
      },
      "outputs": [],
      "source": [
        "# Training and evaluation loop\n",
        "epochs = 400\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}/{epochs},\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    if t >= 19:\n",
        "      val_loss = test(val_loader, model, loss_fn)\n",
        "      scheduler.step(val_loss)\n",
        "\n",
        "    # Checkpointing every 100 epochs\n",
        "    if t % 100 == 0 and t != 0:\n",
        "        torch.save(model.state_dict(), f'model_epoch_{t}.pth')\n",
        "        print(f'Checkpoint saved at epoch {t}')\n",
        "\n",
        "print(\"This marks the end of training the model, now commences the eval stage on the test data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSXwxxL8t98H"
      },
      "source": [
        "## split and format the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ulhNS4PF4XI"
      },
      "source": [
        "## Create a checkpoint for the training and testing in case I have to pause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHW9Rk0dF_u1"
      },
      "source": [
        "## Create a visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZUOZQovuCV_"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylAW-VTxYVp-"
      },
      "outputs": [],
      "source": [
        "model = ConvNN().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNc3ABM20VO3"
      },
      "source": [
        "## Set training parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DprXzJ-23AaY"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uvCNTZg5Rby"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GQvfsvI5TTZ"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU if available\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct / total} %')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzJ9O-vv-IgS"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}